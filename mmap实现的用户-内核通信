用户空间与内核空间的通信方法有很多，如ioctl，procfs，sysfs等。但是，这些方法仅能在用户空间与内核空间之间交互简单的数据。
如果要实现大批量数据的传递，最好的方法就是共享内存。利用设备驱动模型中的mmap函数，可以很容易实现一个简单的共享内存。本文
通过具体实例，介绍一下这种共享内存的实现方法。

系统调用mmap通常用来将文件映射到内存，以加快该文件的读写速度。当用mmap操作一个设备文件时，可以将设备上的存储区域映射到进程
的地址空间，从而以内存读写的方法直接控制设备。如果我们在内核模块里申请了一段内存区域，也可以利用此方法，将这个过程映射到用
户空间，以实现内核空间与用户空间之间的共享内存。

Linux中的每个进程都有一个独立的地址空间，在内核中使用数据结构mm_struct表示。而一个进程的地址空间，由多个vm_area组成。每个
vm_area都映射了一段具体的物理内存空间或IO空间。
我们以图形环境Xorg为例，在/proc/1105(Xorg的PID)/maps文件中可以看到：

b6720000-b6721000 rw-p 0000b000 08:01 5349666    /lib/libnss_files-2.11.so

b6744000-b674d000 r-xp 00000000 08:01 3344582    /usr/lib/xorg/modules/input/evdev_drv.so

b674d000-b674e000 rw-p 00009000 08:01 3344582    /usr/lib/xorg/modules/input/evdev_drv.so

b674e000-b67c7000 rw-p 00000000 00:00 0 

b67c7000-b67c8000 rw-s f8641000 00:0f 11265      /dev/nvidia0

这只是maps文件中的部分节选。其中的每个地址段都对应一个vma。将物理内存映射到用户地址空间的过程，可以概括为2部分。首先，申
一个新的vma。其次为物理内存页面创建页表项，并将该页表项关联到vma上。

在我们调用系统调用mmap时，内核中的sys_mmap函数首先根据用户提供给mmap的参数（如起始地址、空间大小、行为修饰符等）创建新的
vma。然后调用相应文件的file_operations中的mmap函数。如果是设备文件，那么file_operations中的mmap函数由设备驱动的编写者实现。
而在mmap中，我们仅仅需要完成页表项的创建即可。
